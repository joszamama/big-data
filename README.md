# ¡Bienvenido al repositorio de mis proyectos personales de Big Data! Aquí encontrarás diferentes carpetas con distintos contenidos. A continuación, te explicaré cada una de ellas:

1. Carpeta "datasets": Aquí encontrarás distintos datasets para trabajar en tus proyectos. Entre ellos, algunos relacionados con Star Wars, el COVID-19 y camiones. Estos datasets pueden ser utilizados para realizar análisis, entrenar modelos de aprendizaje automático, entre otros usos.

2. Carpeta "docs": En esta carpeta encontrarás toda la documentación y memoria de cada proyecto que hayas realizado. Es importante mantener la documentación actualizada, ya que permitirá a otros usuarios entender mejor tu trabajo y reproducir tus resultados.

3. Carpeta "mapreduce": Aquí encontrarás scripts de Python que han sido utilizados para ejecutar el framework MapReduce. MapReduce es una técnica de procesamiento distribuido de datos que se utiliza en grandes conjuntos de datos. Los scripts en esta carpeta te permitirán aplicar MapReduce a tus propios conjuntos de datos.

4. Carpeta "pig": En esta carpeta encontrarás las consultas empleadas en el framework de procesamiento de datos Pig. Pig es un lenguaje de programación de alto nivel utilizado para analizar grandes conjuntos de datos en Hadoop. Las consultas en esta carpeta pueden ser útiles para realizar análisis de datos en conjuntos grandes de datos.

5. Carpeta "hive": En esta carpeta encontrarás consultas empleadas en el framework de procesamiento de datos Hive. Hive es una herramienta utilizada para analizar grandes conjuntos de datos almacenados en Hadoop. Las consultas en esta carpeta pueden ser útiles para realizar análisis de datos en conjuntos grandes de datos.